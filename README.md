# curxy

#### _cursor_ + _proxy_ = **curxy-static-domain**

This is a fork of [curxy](https://github.com/ryoppippi/curxy) that allows to run a cloudfare tunnel through the cloudflared binary.
The Cloudflare tunnel can be configured through .yml files, which allow you to use static domains.
As a result, the service will be exposed by Cloudflare tunnels on the same persistent domain.

[![JSR](https://jsr.io/badges/@ryoppippi/curxy)](https://jsr.io/@ryoppippi/curxy)
[![JSR](https://jsr.io/badges/@ryoppippi/curxy/score)](https://jsr.io/@ryoppippi/curxy)

An proxy worker for using ollama in cursor

## What is this?

This is a proxy worker for using ollama in cursor. It is a simple server that
forwards requests to the ollama server and returns the response.

## Why do you need this?

When we use llm prediction on cusor editor, the editor sends to the data to the
official cursor server, and the server sends the data to the ollama server.
Therefore, even if the endpoint is set to localhost in the cursor editor
configuration, the cursor server cannot send communication to the local server.
So, we need a proxy worker that can forward the data to the ollama server.

## requirements

- deno
- ollama server

## How to use

1. Launch the ollama server
2. Configure cloudflare:

```
cloudflared tunnel login
cloudflared tunnel create ollama-local
```

3. Retrieve the credentials file path
   A credentials file has been generated by cloudflared, which you can retrieve using `ls ~/.cloudflared/`.

4. Configure tunnel to point to your local app
   Create a config file at `~/.cloudflared/config.yml`:

```yml
tunnel: ollama-local
credentials-file: <absolute-path-to-credentials-file>

ingress:
  - hostname: ollama-local.example.com
    service: http://127.0.0.1:8800
  - service: http_status:404
```

1. Launch curxy

   You can specify the endpoint, port, and tunnel name using the following options:

   ```sh
   deno run dev --port 8800 --tunnel-name ollama-local --subdomain https://ollama-local.example.com
   ```

   If you limit the access to the ollama server, you can set `OPENAI_API_KEY`
   environment variable.

   ```bash
   OPENAI_API_KEY=your_openai_api_key deno run dev --port 8800 --tunnel-name ollama-local --subdomain https://ollama-local.example.com
   ```

   Listening on http://127.0.0.1:62192/
   ‚óê Starting cloudflared tunnel to http://127.0.0.1:62192
   Server running at: https://remaining-chen-composition- dressed.trycloudflare.com

   ```

   You can get the public URL hosted by cloudflare. The tunnel is now started with the specified endpoint, port, and tunnel name.
   ```

2. Enter the URL provided by `curxy` with /v1 appended to it into the "Override
   OpenAl Base URL" section of the cursor editor configuration.

![cursor](https://github.com/user-attachments/assets/83a54310-0728-49d8-8c3f-b31e0d8e3e1b)

7. Add model names you want to "Model Names" section of the cursor editor
   configuration.

![Screenshot 2024-08-22 at 23 42 33](https://github.com/user-attachments/assets/c24fed7c-c61e-46a0-b735-ccf594a96363)

8. (Optional): Additionally, if you want to restrict access to this Proxy Server
   for security reasons, you can set the OPENAI_API_KEY as an environment
   variable, which will enable access restrictions based on the key.

9. **Enjoy!**

Also, you can see help message by `deno run -A jsr:@ryoppippi/curxy --help`

## Related

[Japanese Article](https://zenn.dev/ryoppippi/articles/02c618452a1c9f)

## License

MIT
